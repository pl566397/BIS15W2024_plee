new_sharks %>%
filter(injury!="major*", injury!="minor*", injury!="none*", injury!="fatal*") %>%
count(county, injury) %>%
ggplot(aes(x=county, y=n))+
geom_col()+
coord_flip()+
labs(title="Total Number of Incidents per County")
library("tidyverse")
library("janitor")
library("naniar")
sharks <- read_csv("data/SharkIncidents_1950_2022_220302.csv") %>% clean_names()
glimpse(sharks)
new_sharks<- sharks %>%
filter(incident_num!="NOT COUNTED")
new_sharks<- sharks %>%
filter(incident_num!="NOT COUNTED")
new_sharks %>%
filter(injury!="major*", injury!="minor*", injury!="none*", injury!="fatal*") %>%
count(county, injury) %>%
ggplot(aes(x=county, y=n))+
geom_col()+
coord_flip()+
labs(title="Total Number of Incidents per County")
#San Diego has the highest amount of incidents
library("tidyverse")
library("janitor")
library("naniar")
sharks <- read_csv("data/SharkIncidents_1950_2022_220302.csv") %>% clean_names()
sharks_test <- read_csv("data/SharkIncidents_1950_2022_220302.csv") %>% clean_names()
summary(sharks)
miss_var_summary(sharks)
sharks <- sharks %>%
filter(incident_num != "NOT COUNTED")
sharks_test %>%
filter(!incident_num=="NOT COUNTED")
sharks %>%
group_by(county) %>%
summarise(n = n()) %>%
ggplot(aes(x=reorder(county, n), y=n)) +
geom_col(fill = "#0099f9", alpha=0.8)+
labs(title="Shark Incidents by County (1950-2022)",
x=NULL,
y="n") +
theme(axis.text.x = element_text(angle = 60, hjust = 1),
plot.title = element_text(size = 14, face="bold"))+
geom_text(aes(label = n), vjust = -0.2, size = 3, color = "black")
new_sharks %>%
filter(injury!="major*", injury!="minor*", injury!="none*", injury!="fatal*") %>%
count(month, injury) %>%
ggplot(aes(x=month, y=n))+
geom_col()+
labs(title="Total number of incidents by month",
y="number of incidents")
#month 10 has the highest number of incidents, so October
sharks %>%
count(county) %>%
arrange(desc(n))
sharks %>%
group_by(month) %>%
summarise(total=n(), .groups='keep') %>%
ggplot(aes(x=as_factor(month), y=total))+
geom_col(fill = "#0099f9", alpha=0.8)+
labs(title="Shark Incidents by Month",
x="Month",
y="n")+
theme(plot.title = element_text(size = 14, face="bold"))
sharks %>%
tabyl(county, injury) %>%
adorn_totals("col") %>%
arrange(desc(fatal)) %>%
as_tibble()
new_sharks %>%
count(county, injury)
sharks %>%
group_by(county, injury) %>%
summarise(total=n(), .groups='keep') %>%
pivot_wider(names_from = injury, values_from = total) %>%
mutate(total=sum(minor, major, fatal, none, na.rm=T)) %>%
arrange(desc(fatal))
new_sharks %>%
count(county, injury) %>%
filter(injury=="fatal")
#San Luis Obispo has the highest number of fatalities
sharks %>%
count(mode) %>%
arrange(desc(n))
new_sharks %>%
group_by(mode) %>%
ggplot(aes(x=mode, y=injury))+
geom_col()+
coord_flip()+
labs(title="Total Number of Incidents per County")
#surfing and boarding are associated with the highest number of incidents
sharks %>%
ggplot(aes(x=injury, fill=injury))+
geom_bar(alpha=0.8, position="dodge")+
facet_wrap(~mode)+
labs(title="Injury Type by Activity",
x=NULL,
y="Number of Incidents")+
theme(strip.text = element_text(size=10),
axis.text.x = element_text(size=8, angle = 60, hjust = 1))
new_sharks %>%
ggplot(aes(x=injury, y=injury))+
geom_col()+
labs(title="Numbers and Types of Injuries by Mode")+
facet_wrap(~mode)+
theme(axis.text.x = element_text(angle = 60, hjust=1))
sharks %>%
count(species) %>%
arrange(desc(n))
new_sharks %>%
ggplot(aes(x=species, y=injury))+
geom_col()+
labs(title="Shark Species by Number of Incidents")
#great white shark species has the highest number of incidents
sharks %>%
filter(species=="White") %>%
ggplot(aes(x=injury))+
geom_bar(fill = "#0099f9", alpha=0.8)+
labs(title="Incidents Involving Great White Sharks",
x="Injury",
y="n")+
theme(axis.text.x = element_text(angle = 60, hjust = 1),
plot.title = element_text(size = 14, face="bold"))
new_sharks %>%
filter(injury=="fatal") %>%
ggplot(aes(x=species, y=injury, fill=injury))+
geom_col()+
labs(title="Fatal Injuries by Species")
#yes all incidents involving the Great White is fatal
white_sharks <- read_csv("data/White sharks tracked from Southeast Farallon Island, CA, USA, 1999 2004.csv", na = c("?", "n/a")) %>% clean_names()
glimpse(white_sharks)
white_sharks %>%
count(sex, total_length_cm)
white_sharks <- read_csv("data/White sharks tracked from Southeast Farallon Island, CA, USA, 1999 2004.csv", na = c("?", "n/a")) %>% clean_names()
glimpse(white_sharks)
miss_var_summary(white_sharks)
white_sharks %>%
filter(sex!="NA") %>%
group_by(sex) %>%
summarise(mean_length=mean(total_length_cm, na.rm=T),
n=n(), .groups='keep')
white_sharks %>%
filter(sex!="NA") %>%
ggplot(aes(x=sex, y=total_length_cm, fill=sex))+
geom_boxplot(alpha=0.8)+
labs(title="Length of Great White Sharks by Sex",
x="Sex",
y="Total Length (cm)")
white_sharks%>%
filter(sex!="NA") %>%
ggplot(aes(x=sex, y=total_length_cm))+
geom_col()+
labs(title="Range of Total Length by Sex")
white_sharks %>%
filter(sex!="NA") %>%
ggplot(aes(x=total_length_cm))+
geom_density(fill = "#0099f9", alpha=0.8)+
facet_wrap(~sex)+
labs(title="Distribution of Total Length by Sex",
x="Total Length (cm)",
y=NULL)+
theme(strip.text = element_text(size=10),
axis.text.x = element_text(size=8, angle = 60, hjust = 1))
#Using the data "white_sharks", how many sharks were females and mature
white_sharks %>%
filter(maturity=="Mature", sex!="NA") %>%
ggplot(aes(x=sex, y=maturity))+
geom_col()+
coord_flip()+
labs(title="The sex of sharks by maturity")
new_sharks %>%
filter(injury!="major*", injury!="minor*", injury!="none*", injury!="fatal*") %>%
count(county, injury) %>%
ggplot(aes(x=county, y=n))+
geom_col()+
coord_flip()+
labs(title="Total Number of Incidents per County")
#San Diego has the highest amount of incidents
new_sharks %>%
count(county, injury) %>%
filter(injury=="fatal")
#San Luis Obispo has the highest number of fatalities
new_sharks %>%
group_by(mode) %>%
ggplot(aes(x=mode, y=injury))+
geom_col()+
coord_flip()+
labs(title="Total Number of Incidents per County")
#surfing and boarding are associated with the highest number of incidents
new_sharks %>%
ggplot(aes(x=injury, y=injury))+
geom_col()+
labs(title="Numbers and Types of Injuries by Mode")+
facet_wrap(~mode)+
theme(axis.text.x = element_text(angle = 60, hjust=1))
new_sharks %>%
ggplot(aes(x=species, y=injury))+
geom_col()+
labs(title="Shark Species by Number of Incidents")
#great white shark species has the highest number of incidents
new_sharks %>%
filter(injury=="fatal") %>%
ggplot(aes(x=species, y=injury, fill=injury))+
geom_col()+
labs(title="Fatal Injuries by Species")
#yes all incidents involving the Great White is fatal
glimpse(white_sharks)
white_sharks %>%
count(sex, total_length_cm)
white_sharks%>%
filter(sex!="NA") %>%
ggplot(aes(x=sex, y=total_length_cm))+
geom_col()+
labs(title="Range of Total Length by Sex")
white_sharks%>%
filter(sex!="NA") %>%
ggplot(aes(x=sex, y=total_length_cm))+
geom_boxplot()+
labs(title="Range of Total Length by Sex")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library("tidyverse")
library("janitor")
library("lubridate") #this will help us manage dates
files <- list.files(path = "data/spiders", pattern = ".csv", full.names = TRUE) #how to upload multiple files together
files
files <- list.files(path = "data/spiders", pattern = ".csv", full.names = F) #how to upload multiple files together
files
spider_list <- lapply(files, read_csv)
getwd()
files <- list.files(path = "data/spiders", pattern = ".csv", full.names = T) #how to upload multiple files together, full.names=T > gives full names, F gives names of files only
files
spider_list <- lapply(files, read_csv) #look into files and apply a read csv to each
spider_list <- lapply(files, read_csv) #look into files and apply a read csv to each
spider_list[[3]] #can look at elements in the list, look at the 3rd element in the list
names(spider_list)
names(spider_list)
str(spider_list)
glimpse(spider_list)
names(spider_list[[1]])#use double brackets,
names <- list.files(path = "data/spiders", pattern = ".csv")
names
names_list <- strsplit(names, split = " .csv")#look at the files in this packet and add the names but separate names from " .csv"
names_list
names_vec <- unlist(names_list) #make a vector of all the names
names_vec
names(spider_list) <- names_vec
names(spider_list)
Butte
butte <- spider_list[["Butte"]]
spider_list[["Butte"]]
butte<- spider_list[["Butte"]] #instead of using number, we can just use the name of county
spiders_all <- bind_rows(spider_list) #every single spider listed has a unique number, bindrows and combine all thoe 30 separate files into one
spiders_all
table_A <- read_csv("data/table_A.csv")
table_B <- read_csv("data/table_B.csv")
table_A
table_B
#join_type(firstTable, secondTable, by=columnTojoinOn)
join_type(firstTable, secondTable, by=columnTojoinOn) #we want to take both datasets and join tem together even though these datasets have different information, join first dataset, second data and by column
inner_exampleDF <- inner_join(table_A, table_B, by="customer_ID")
inner_exampleDF
left_exampleDF <- left_join(table_A, table_B, by="customer_ID")
left_exampleDF
right_exampleDF <- right_join(table_A, table_B, by="customer_ID")
right_exampleDF#right join does the same thing but customer 3 is dropped and we have missing data for 4
full_exampleDF <- full_join(table_A, table_B, by="customer_ID")
full_exampleDF#full join does not care for left or right, we have all customers but just have missing data for 3 and 4
anti_exampleDF <- anti_join(table_A, table_B, by="customer_ID")
anti_exampleDF#provide the rows in the first table, can use this to troubleshoot
spiders_locs <- read_csv("data/spiders locations/spiders_locations.csv")
spiders_locs <- read_csv("data/spiders locations/spiders_locations.csv")
spiders_locs
spiders_all
spiders_with_locs <-
left_join(spiders_all, spiders_locs, by="Accession")
summary(spiders_with_locs)
spiders_with_locs <-
full_join(spiders_all, spiders_locs, by="Accession") #to spiders all, join spiders locs by the unie number Accesion, now we see that Lat. and Long. is joined, smush the new data onto the old data, you want to tag it onto the end of the old dataset so it has to be a left join
summary(spiders_with_locs)
class(spiders_with_locs$Date)
glimpse(spiders_with_locs)
View(spiders_with_locs)
day <- today()
day
str(day)
datetime <- now()
datetime
dmy(spiders_with_locs$Date)
dmy(spiders_with_locs$Date) #dmy converts slashes into a date that R can read, anytime there is a weird format, though it is going to erase the date and put an NA there
dateformat1 <- "20200922"
dateformat2 <- "09-22-2020"
dateformat3 <- "22/09/2020"
dateformat4 <- "09-22-2020 17:00:00"
dateformat5 <- "20200922 170000"
ymd(dateformat1)
mdy(dateformat2)
dmy(dateformat3)
dmy_hms(dateformat4)
mdy_hms(dateformat4)
ymd_hms(dateformat5)
write.csv(spiders_with_locs, file = "spiders_with_locs.csv", row.names = FALSE)
library(tidyverse)
library(janitor)
#install.packages("ggmap")
library(ggmap)
register_stadiamaps("e77f55a8-a371-44cd-a7dd-6384b4586d64", write = FALSE)
library(tidyverse)
library(janitor)
#install.packages("ggmap")
library(ggmap)
install.packages("ggmap")
library(ggmap)
register_stadiamaps("e77f55a8-a371-44cd-a7dd-6384b4586d64", write = FALSE)
register_stadiamaps("e77f55a8-a371-44cd-a7dd-6384b4586d64", write = FALSE) #a free personalized key for
register_stadiamaps("e77f55a8-a371-44cd-a7dd-6384b4586d64", write = FALSE) #a free personalized key for
spiders <- read_csv("data/spiders_with_locs.csv")%>% clean_names()
spiders <- spiders %>% filter(latitude<=42)
spiders %>%
select(latitude, longitude) %>%
summary()#where on earth we want to orient our map
lat <- c(34.67, 41.80)
long <- c(-124.1, -115.5)
bbox <- make_bbox(long, lat, f = 0.03) #f is the fraction of the bounding box to add to the range
lat <- c(34.67, 41.80)#the min and max for lat
long <- c(-124.1, -115.5)#the min and max for lat
bbox <- make_bbox(long, lat, f = 0.03) #f is the fraction of the bounding box to add to the range
map1 <- get_stadiamap(bbox, maptype = "stamen_terrain", zoom=7) #get stamen map, use bbox to make map, very senitive t zoom so keep it at zoom=7
ggmap(map1)
ggmap(map1) +
geom_point(data = spiders, aes(longitude, latitude), size=0.4) +
labs(x= "Longitude", y= "Latitude", title="Spider Locations")#take the lat and long and put it on the map
sharks <- read_csv("data/SharkIncidents_1950_2022_220302.csv") %>%
clean_names() %>%
filter(longitude !="NA" & latitude !="NA") %>% # pulling out NA locations
mutate(longitude = as.numeric(longitude)) # converting longitude to numeric
sharks_dups <- sharks %>%
distinct(location, .keep_all = TRUE) # remove duplicate locations, but keep the remaining variables
sharks %>%
select(latitude, longitude) %>%
summary()
lat <- c(32.59, 41.56)#the min and max for lat
long <- c(-124.7, -117.1)#the min and max for lat
bbox <- make_bbox(long, lat, f = 0.03)
map1 <- get_stadiamap(bbox, maptype = "stamen_terrain", zoom=7)
map_sharks <- get_stadiamap(bbox, maptype = "stamen_terrain", zoom=7)
ggmap(map_sharks)
ggmap(map1) +
geom_point(data = sharks, aes(longitude, latitude), size=0.4) +
labs(x= "Longitude", y= "Latitude", title="Shark Locations")
sharks_dups %>%
select(latitude, longitude) %>%
summary()
lat <- c(32.59, 41.56)#the min and max for lat
long <- c(-124.7, -117.1)#the min and max for lat
bbox <- make_bbox(long, lat, f = 0.03)
map_sharks <- get_stadiamap(bbox, maptype = "stamen_terrain", zoom=7)
ggmap(map_sharks)
ggmap(map1) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.4) +
labs(x= "Longitude", y= "Latitude", title="Shark Locations")
sharks_dups %>%
select(fatal, latitude, longitude) %>%
summary()
View(sharks_dups)
sharks_dups %>%
filter(injury=="fatal")
select(latitude, longitude) %>%
summary()
sharks_dups %>%
filter(injury=="fatal") %>%
select(latitude, longitude) %>%
summary()
lat <- c(32.85, 39.58)#the min and max for lat
long <- c(-123.8, -117.3)#the min and max for lat
bbox <- make_bbox(long, lat, f = 0.03)
map_fatal <- get_stadiamap(bbox, maptype = "stamen_terrain", zoom=7)
ggmap(map_fatal)
ggmap(map_fatal) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.4) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
ggmap(map_sharks) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.4) +
labs(x= "Longitude", y= "Latitude", title="Shark Locations")
ggmap(map_sharks) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Shark Locations")
ggmap(map_sharks) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.1) +
labs(x= "Longitude", y= "Latitude", title="Shark Locations")
ggmap(map_sharks) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.3) +
labs(x= "Longitude", y= "Latitude", title="Shark Locations")
ggmap(map_fatal) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.1) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
ggmap(map_fatal) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
ggmap(map_fatal) +
geom_point(data = sharks_fatal, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
ggmap(map_fatal) +
geom_point(data = sharks_dups, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
sharks_fatal<- sharks_dups %>%
filter(injury=="fatal") %>%
select(latitude, longitude) %>%
summary()
lat <- c(32.85, 39.58)#the min and max for lat
long <- c(-123.8, -117.3)#the min and max for lat
bbox <- make_bbox(long, lat, f = 0.03)
ggmap(map_fatal) +
geom_point(data = sharks_fatal, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
ggmap(map_fatal) +
geom_point(data = sharks_fatal, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
ggmap(map_fatal) +
geom_point(sharks_fatal, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
#ggmap(map_fatal) +
#geom_point(data= sharks_fatal, aes(longitude, latitude), size=0.8) +
labs(x= "Longitude", y= "Latitude", title="Fatal Shark Attack Locations")
